DATA MODELS AND QUERY LANGUAGES --> The format in which you give the database the data
and the mechanism by which you can ask it again later

Modeling data is one of the most crucial aspects of software development.
A well-designed data model enhances our understanding of the problem and
profoundly impacts the software development process.

Layered Approach to Data Modeling
Applications are typically built by layering one data model on top of another.
The key question at each layer is:
How is this data represented in terms of the next-lower layer?
In complex applications, multiple intermediary layers‚Äîsuch as APIs built on top of
other APIs‚Äîhelp abstract underlying complexities.
These abstractions allow different teams
(e.g., database engineers and application developers) to collaborate effectively without needing deep expertise in all layers.
Applications are built by layering by one data model on top of another
For each layer, the key question is: how is it represented in terms of the next-lower layer? For
In a complex application there may be more intermediary levels, such as APIs built
upon APIs, but the basic idea is still the same: each layer hides the complexity of the
layers below it by providing a clean data model. These abstractions allow different
groups of people‚Äîfor example, the engineers at the database vendor and the application
developers using their database‚Äîto work together effectively.

The Relational Model and SQL
The most well-known data model today is SQL, based on the relational model proposed by
Edgar Codd in 1970. In this model:
Data is structured into relations (tables in SQL).
Each relation consists of an unordered collection of tuples (rows in SQL).
By the mid-1980s, relational database management systems (RDBMS) and SQL became
the standard tools for storing and querying structured data.
Their dominance has lasted for 25‚Äì30 years‚Äîan eternity in computing history.

Origins of Relational Databases
Relational databases originated in business data processing on mainframe
computers in the 1960s and ‚Äô70s. Early use cases included:

Transaction processing: Handling sales, banking transactions, airline reservations,
and inventory management.
Other databases at that time forced application developers to think a lot about the
internal representation of the data in the database. The goal of the relational model
was to hide that implementation detail behind a cleaner interface.

Batch processing: Customer invoicing, payroll, and reporting.
Even though these use cases may seem mundane today, relational databases provided a
structured and reliable way to manage critical business operations.
Their principles continue to influence modern database systems.

NoSQL (Not only SQL ) born in 2010 out of a meetup..
a distributed open source non-relational database
Driving forces behind the adaptation
‚Ä¢ A need for greater scalability than relational databases can easily achieve, includ‚Äê
ing very large datasets or very high write throughput
‚Ä¢ A widespread preference for free and open source software over commercial
database products
‚Ä¢ Specialized query operations that are not well-supported by the relational model
‚Ä¢ Frustration with the restrictiveness of relational schemas, and a desire for a more
dynamic and expressive data model [5]

The main arguments in favor of the document data model are schema flexibility, bet‚Äê
ter performance due to locality, and that for some applications it is closer to the data
structures used by the application. The relational model counters by providing better
support for joins, and many-to-one and many-to-many relationships.


Object-relational mapping (ORM) frameworks like ActiveRecord and Hibernate
reduce the amount of boilerplate code required for this translation layer - objects in the application
code and database model of tables - rows and columns, but they can‚Äôt completely hide the differences between the two models
called as the impedance mismatch

For a data structure like a r√©sum√©, which is mostly a self-contained document, a JSON
representation can be quite appropriate JSON has the appeal of
being much simpler than XML.It can reduce the impedance mismatch
The one-to-many relationships from the user profile to the user‚Äôs positions, educa‚Äê
tional history, and contact information imply a tree structure in the data, and the
JSON representation makes this tree structure explicit

Explains why region_id and industry_id are stored as IDs instead of plain-text strings
like "Greater Seattle Area" and "Philanthropy". Using IDs provides several advantages
over free-text input, particularly when data consistency and usability are important.
Benefits of Using Standardized IDs Instead of Plain-Text Strings

Consistency in Style and Spelling
Users may enter the same region or industry differently
(e.g., "Philanthropy" vs. "Nonprofit Work").
Standardized IDs ensure uniform naming across all profiles.

Avoiding Ambiguity
Some cities share the same name (e.g., Portland, OR vs. Portland, ME).
Using region IDs removes confusion by linking entries to a unique location.

Ease of Updating
If a region or industry name changes (e.g., due to political events), updating the name
in one place updates all references.
No need to modify individual user profiles.

Localization Support
Standardized lists allow seamless translation when the site is available in multiple
languages.
The stored region ID ensures that a user in France sees "Grand Seattle" instead of
"Greater Seattle Area".

Improved Search Capabilities
Structured data enables hierarchical relationships, such as recognizing that
Seattle is part of Washington State.
A search for philanthropists in Washington will correctly include profiles
tagged with Greater Seattle Area.

Conclusion
While free-text input may be more flexible for users,
structured IDs provide greater data integrity, usability, and maintainability.
By offering predefined options in a dropdown list or autocomplete field,
applications enhance user experience while ensuring clean, searchable,
and localized data.

The advantage of using an ID is that because it has no meaning to humans, it never
needs to change: the ID can remain the same, even if the information it identifies
changes. Anything that is meaningful to humans may need to change sometime in
the future‚Äîand if that information is duplicated, all the redundant copies need to be
updated. That incurs write overheads, and risks inconsistencies (where some copies
of the information are updated but others aren‚Äôt). Removing such duplication is the
key idea behind normalization in databases.

Unfortunately, normalizing this data requires many-to-one relationships (many peo‚Äê
ple live in one particular region, many people work in one particular industry), which
don‚Äôt fit nicely into the document model. In relational databases, it‚Äôs normal to refer
to rows in other tables by ID, because joins are easy. In document databases, joins are
not needed for one-to-many tree structures, and support for joins is often weak.

If the database itself does not support joins, you have to emulate a join in application
code by making multiple queries to the database. (In this case, the lists of regions and
industries are probably small and slow-changing enough that the application can
simply keep them in memory. But nevertheless, the work of making the join is shifted
from the database to the application code.)

Use organizations as entities
Recommendation - many to many features

While many-to-many relationships and joins are routinely used in relational data‚Äê
bases, document databases and NoSQL reopened the debate on how best to represent
such relationships in a database.

One to many relationships work well in Document DB/NoSQL
Many to many relationships are difficult in NoSQL as such DB's do not support joins

Hierarichal model similar to JSON data structure
Then there was the network model-CODASYL

Document databases reverted back to the hierarchical model in one aspect: storing
nested records (one-to-many relationships, like positions, education, and
contact_info) within their parent record rather than in a separate
table.However, when it comes to representing many-to-one and many-to-many relation‚Äê
ships, relational and document databases are not fundamentally different: in both
cases, the related item is referenced by a unique identifier, which is called a foreign
key in the relational model and a document reference in the document model.
That identifier is resolved at read time by using a join(SQL) or follow-up queries(noSQL).

In a relational database, the query optimizer automatically decides which parts of the
query to execute in which order, and which indexes to use. Those choices are effec‚Äê
tively the ‚Äúaccess path,‚Äù but the big difference is that they are made automatically by
the query optimizer, not by the application developer, so we rarely need to think
about them.

If you want to query your data in new ways, you can just declare a new index, and
queries will automatically use whichever indexes are most appropriate. You don‚Äôt
need to change your queries to take advantage of a new index.
The relational model thus made it much easier to add new features to applications.

Query optimizers for relational databases are complicated beasts, and they have con‚Äê
sumed many years of research and development effort. But a key insight of the
relational model was this: you only need to build a query optimizer once, and then all
applications that use the database can benefit from it. If you don‚Äôt have a query opti‚Äê
mizer, it‚Äôs easier to handcode the access paths for a particular query than to write a
general-purpose optimizer‚Äîbut the general-purpose solution wins in the long run.


STORAGE AND RETRIEVAL->how we can store the data that we‚Äôre given, and how we
can find it again when we‚Äôre asked for it from the database's point of view
how the database handles storage and retrieval internally
Why is it important as an app developer?
Because you do need to select a storage engine that is appropriate for
your application, from the many that are available. In order to tune a storage engine
to perform well on your kind of workload, you need to have a rough idea of what the
storage engine is doing under the hood.

In particular, there is a big difference between storage engines that are optimized for
transactional workloads and those that are optimized for analytics
‚ÄúColumn-Oriented Storage‚Äù- discuss a family of storage engines that
is optimized for analytics.

Log structured storage engines and page-oriented storage engines such as B Trees
db_set () {
 echo "$1,$2" >> database
}
db_get () {
 grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}

Similarly to what
db_set does, many databases internally use a log, which is an append-only data file.
Real databases have more issues to deal with (such as concurrency control, reclaim‚Äê
ing disk space so that the log doesn‚Äôt grow forever, and handling errors and partially
written records), but the basic principle is the same. Logs are incredibly useful
The word log is often used to refer to application logs, where an
application outputs text that describes what‚Äôs happening. Here, log is used in the more general sense: an append-only
sequence of records. It doesn‚Äôt have to be human-readable; it might
be binary and intended only for other programs to read.

On the other hand, our db_get function has terrible performance if you have a large
number of records in your database. Every time you want to look up a key, db_get
has to scan the entire database file from beginning to end, looking for occurrences of
the key. In algorithmic terms, the cost of a lookup is O(n): if you double the number
of records n in your database, a lookup takes twice as long. That‚Äôs not good.
In order to efficiently find the value for a particular key in the database, we need a
different data structure: an index. In this chapter we will look at a range of indexing
structures and see how they compare; the general idea behind them is to keep some
additional metadata on the side, which acts as a signpost and helps you to locate the
data you want. If you want to search the same data in several different ways, you may
need several different indexes on different parts of the data.
An index is an additional structure that is derived from the primary data. Many data‚Äê
bases allow you to add and remove indexes, and this doesn‚Äôt affect the contents of the
database; it only affects the performance of queries. Maintaining additional structures
incurs overhead, especially on writes. For writes, it‚Äôs hard to beat the performance of
simply appending to a file, because that‚Äôs the simplest possible write operation. Any
kind of index usually slows down writes, because the index also needs to be updated
every time data is written.
This is an important trade-off in storage systems: well-chosen indexes speed up read
queries, but every index slows down writes. For this reason, databases don‚Äôt usually
index everything by default, but require you‚Äîthe application developer or database
administrator‚Äîto choose indexes manually, using your knowledge of the applica‚Äê
tion‚Äôs typical query patterns. You can then choose the indexes that give your ap

Index ..like a hashstructure, dictionary
keep an in memory hash map every key to byte offset in the data file- the location at which the
record can be found
When you append new data to the log, also add an entry in the hashmap to reflect
the offset of the data you just wrote
When you want to lookup a value, use the hashmap and the key to get the offset in the
data file, seek to that location and read the value


In earlier days of business data processing, a write to the database involved
a commercial transaction taking place such as placing an order with a supplier,
making a sala,paying an employer's salary. That term  transaction has stuck behind
referring to a group of reads and writes that forms a logical unit


A transaction needn‚Äôt necessarily have ACID (atomicity, consis‚Äê
tency, isolation, and durability) properties. Transaction processing
just means allowing clients to make low-latency reads and writes‚Äî
as opposed to batch processing jobs, which only run periodically
(for example, once per day).

Over time the use of databases has been expanded for eg social media
and the databases started being used for different kinds of application and data
the basic access pattern remained similar to business transactions
-An application looks up a small number of records by some key using an index
Records are inserted or updated based on the user‚Äôs input.
Because these applications are interactive, the access pattern became known as online
transaction processing(OLTP).

However, databases also started being increasingly used for data analytics, which has
very different access patterns. Usually an analytic query needs to scan over a huge
number of records, only reading a few columns per record, and calculates aggregate
statistics (such as count, sum, or average) rather than returning the raw data to the
user. For example, if your data is a table of sales transactions, then analytic queries
might be:
What was the total revenue of each of our stores in January?
‚Ä¢ How many more bananas than usual did we sell during our latest promotion?
‚Ä¢ Which brand of baby food is most often purchased together with brand X
diapers?

These queries are often written by business analysts, and feed into reports that help
the management of a company make better decisions (business intelligence). In order
to differentiate this pattern of using databases from transaction processing, it has
been called online analytic processing (OLAP)


OLTP (Property Transaction Processing Systems) vs. OLAP (Analytical Systems)

Feature                | OLTP (Transactional Systems)                 | OLAP (Analytical Systems)
---------------------- | -------------------------------------------- | ------------------------------
Main Read Pattern      | Fetches a small number of records per query, usually by key. | Aggregates over a large number of records for analysis.
Main Write Pattern     | Handles random-access, low-latency writes from user input.  | Writes occur in bulk imports (ETL) or continuous event streams.
Primary Users         | End users/customers interacting via a web or mobile application. | Internal analysts using data for business intelligence and decision-making.
Data Representation   | Stores the latest state of data, reflecting the current point in time. | Stores historical events, capturing changes over time.
Dataset Size          | Typically gigabytes to terabytes. | Can scale from terabytes to petabytes.

Key Takeaways:
1. OLTP systems are optimized for fast, concurrent transactions, making them ideal for applications like banking, e-commerce, and inventory management.
2. OLAP systems support complex queries and large-scale data analysis, commonly used for reporting, trend analysis, and business intelligence.
3. ETL (Extract, Transform, Load) processes are central to OLAP, ensuring that structured transactional data is converted into meaningful insights.

TRANSACTION
1)Database software or hardware may fail in the middle of transactions--->lets say during write
2)Application may crash at any time halfway through series of operations
3)Network failures or interruptions can cut the application from database or one
database node from another
4)Several clients may write to database at the same time overwriting each other's changes
5)Client may read partially updated data
6)Race condition may cause suprising bugs
In order to be reliable, a system has to deal with these faults and ensure that they
don‚Äôt cause catastrophic failure of the entire system.
However, implementing fault tolerance mechanisms is a lot of work.
It requires a lot of careful thinking about all
the things that can go wrong, and a lot of testing to ensure that the solution actually
works.

For decades, transactions have been the mechanism of choice for simplifying these issues. A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails(abort, rollback). If it fails, the application can safely retry. With transactions, error
handling becomes much simpler for an application, because it doesn‚Äôt need to worry about partial failure‚Äîi.e., the case where some operations succeed and some fail (for whatever reason).

Transactions are not a law of nature; they were cre ated with a purpose, namely to simplify the programming model for applications
accessing a database. By using transactions, the application is free to ignore certain potential error scenarios and concurrency issues, because the database takes care of them instead (we call these safety guarantees).
Not every application needs transactions, and sometimes there are advantages to weakening transactional guarantees or abandoning them entirely (for example, to achieve higher performance or higher availability). Some safety properties can be achieved without transactions.

Safety guarantee provided by transactions comes in the way of scalability
The safety guarantees provided by transactions are often described by the well known acronym ACID,which stands for Atomicity, Consistency, Isolation, and Durability

In the late 2000s, nonrelational (NoSQL) databases started gaining popularity. They aimed to improve upon the relational status quo by offering a choice of new data model, and by including replication and partitioning by default. Transactions were the main casualty of this movement: many
of this new generation of databases abandoned transactions entirely, or redefined the word to describe a much weaker set of guarantees than had previously been understood.

#Atomicity
Atomicity, in computing, refers to an operation that is indivisible and cannot be partially completed. In multi-threaded programming, it ensures that an operation appears as a single, uninterruptible step to other threads.
For example, in multi-threaded programming, if one thread executes an atomic operation, that means there is no way that another thread could see the half-finished result of the operation. The system can only be in the state it was before the operation or after the operation, not something in between.
However, in the context of ACID transactions, atomicity ensures that if a transaction consisting of multiple writes
fails due to a system crash, network failure, or constraint violation, all partial changes are discarded.
This prevents inconsistent or duplicate data, allowing the application to safely retry the transaction without uncertainty.
In other words database saves you from having to worry about partial failure, by giving an all in or nothing guarantee
The defining feature of atomicity is abortability --the ability to abort a transaction entirely if it cannot be fully completed.

#Consistency
In the context of ACID, consistency refers to an application-specific notion of the
database being in a ‚Äúgood state.‚Äù
ACID consistency ensures that certain data invariants‚Äîrules that must always hold true‚Äîare maintained. For example, in an accounting system, total credits and debits must always balance. However, maintaining consistency is primarily the application's responsibility, not the database‚Äôs. While databases can enforce some constraints (e.g., foreign keys, uniqueness rules), they cannot prevent application logic errors from introducing invalid data. Unlike atomicity, isolation, and durability, which are database properties, consistency in ACID is an application-level concern, making its inclusion in ACID somewhat debatable.
Thus, some argue that C in ACID doesn‚Äôt truly belong, as databases only store data, while applications define what is valid.
Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application. 

#Isolation
ACID isolation ensures that concurrently executing transactions do not interfere with each other. The formal definition of isolation is serializability, meaning each transaction behaves as if it were the only one running. Even if transactions execute concurrently, the database guarantees that the final result (once all transactions are committed, ) is equivalent to running them sequentially (one after another).
and that the outcome matches that of a sequential execution

Case 1: Weak Isolation (Read Committed or Repeatable Read)
Bob starts a transaction and sees $1,000.
At the same time, Alice is withdrawing $500 but hasn‚Äôt committed yet.
Bob decides to withdraw $700 because he sees $1,000.
If the database allows concurrent transactions without strong isolation, Bob‚Äôs withdrawal could be processed before Alice‚Äôs withdrawal is committed, leading to overdraft issues.

Case 2: Strong Isolation (Serializable) ‚Äì Prevents This Issue
To prevent this, the database can use locking mechanisms or optimistic concurrency control:
Locking (Pessimistic Concurrency Control)
When Alice starts withdrawing $500, the database locks her account balance row.
Bob has to wait until Alice‚Äôs transaction is committed or rolled back.
If Alice‚Äôs transaction commits, Bob will now see $500 available and can no longer withdraw $700.

Optimistic Concurrency Control (Used in Some Databases)
Bob reads the balance as $1,000 and tries to withdraw $700.
When Bob‚Äôs transaction tries to commit, the database checks if the balance is still $1,000.
If Alice‚Äôs withdrawal has already been committed, the database detects the conflict and reject's Bob transaction

However, in practice, serializable isolation is rarely used, because it carries a performance penalty. Some popular databases, such as Oracle 11g, don‚Äôt even implement it.
In Oracle there is an isolation level called ‚Äúserializable,‚Äù but it actually implements
something called snapshot isolation, which is a weaker guarantee than serializability
	

Durability
The purpose of a database system is to provide a safe place where data can be stored without fear of losing it. Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.

In a single-node database, durability typically means that the data has been written to nonvolatile storage such as a hard drive or SSD. It usually also involves a write-ahead log (transaction) or similar which allows recovery in the event that the data structures on disk are corrupted. 

In a replicated database, durability may mean that the data has been successfully copied to some number of nodes. 
In order to provide a durability guarantee, a database must wait until these writes or replications are complete before reporting a transaction as successfully committed.
Perfect durability does not exist: if all your hard disks and all your backups are destroyed at the same time, there‚Äôs obviously
nothing your database can do to save you.

In practice, there is no one technique that can provide absolute guarantees. There are
only various risk-reduction techniques, including writing to disk, replicating to
remote machines, and backups‚Äîand they can and should be used together. As always, it‚Äôs wise to take any theoretical ‚Äúguarantees‚Äù with a healthy grain of salt.

Recap - broader picture..
Atomicity
If an error occurs halfway through a sequence of writes, the transaction should be aborted, and the writes made up to that point should be discarded. In other words, the database saves you from having to worry about partial failure, by giving an all-or-nothing guarantee.

Isolation
Concurrently running transactions shouldn‚Äôt interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.	

Scenario 1
Email table, counts of unread message table
An insert is made to email table... before the count is updated..

The user queries the email and sees the new unread email but the count still shows previous or 0-- incorrect
-- Violating isolation: one transaction reads another transaction‚Äôs uncommitted writes (a ‚Äúdirty read‚Äù).
-- Isolation guarantee would have prevented that by ensuring that user sees either both the inserted email and the updated counter, or neither, but not an inconsistent halfway point. You can roll both operation in a transaction


Here you need atomicity
if an error occurs somewhere over the course of the transaction, the contents of the mailbox and the unread counter might
become out of sync. In an atomic transaction, if the update to the counter fails, the transaction is aborted and the inserted email is rolled back.
Atomicity ensures that if an error occurs any prior writes from that transaction are undone, to avoid an inconsistent state.


Multi-object transactions require some way of determining which read and write
operations belong to the same transaction. In relational databases, that is typically
done based on the client‚Äôs TCP connection to the database server: on any particular connection, everything between a BEGIN TRANSACTION and a COMMIT statement is considered to be part of the same transaction.

Challenge in Joins?



### **Batch Processing vs. Stream Processing**

#### **Batch Processing**
- Involves accumulating data over a fixed interval (e.g., daily) and processing it together.
- Data is **bounded** (of a known and finite size).The batch process knows when it has finished reading its inputs
--For eg the sorting operation central to Map-Reduce must read the entire input before it can start producing output
- Processing time can vary, but the output is available only after a certain delay.

- Example: Credit card usage reports (e.g., rolling last 30-day report).
- **Input & Output:** Typically files.
- **Processing Flow:**
  - Parse the file into a sequence of records	
- **Reliability:**
  - Failed tasks are automatically retried, and partial outputs from failed tasks are discarded, ensuring consistent results.
  - This simplifies the programming model.
- In reality , the data is unbounded- it arrives gradually over time. Batch processors must artifically divide data into chunks of fixed duration
- Processing a day's worth of data at end of every day or processing an hour's worth of data at end of every hour
- Changes in input are reflected in the output a day later
- To reduce delay, run processing more frequently-- processing a second's worth of data at end of every second
- or abandoning the fixed time slices entirely and simply processing each event as it happens. That is the idea behind stream processing
---

#### **Stream Processing**

- Processes data continuously in real-time, removing fixed intervals.
- Stream refers to data that is incrementally made available over time
- Handles each event as it arrives, making the data **unbounded** (continuous stream).
- **Example:** Stock market price updates, real-time fraud detection.
- **Event Representation:**
  - In context of stream processing, records are more commonly known as events
	But it is essentially the same thing - Event is a small self contained immutable objects containing details of an occurrence and its timestamp.
  - Can be encoded as JSON, text, or binary.
- **Storage & Transport:**
  - Events can be stored in a file, relational table, or document database.
  - Events can be transmitted over the network for further processing.

##### **Key Differences from Batch Processing:**
- In batch processing, a file is written once and read multiple times.
- In streaming, an event is generated once by a producer (publisher/sender) and consumed by multiple consumers(subscriptions/recipients).
- In a filesystem, a filename identifies a set of related records; in a streaming system, related events are usually grouped together
	into a topic or stream.


##### **Challenges in Streaming Systems:**
- In principle, a file or database is sufficient to connect producers and consumers: a producer writes every event that it generates to the  datastore, and each consumer periodically polls the datastore to check for events that have appeared since it last ran.
-However, when moving toward continual processing with low delays, polling becomes expensive if the datastore is not designed for this kind of usage. The more often you poll, the lower the percentage of requests that return new events, and thus
the higher the overheads become. Instead, it is better for consumers to be notified when new events appear
- Databases are very limited in what they can do as part of polling mechanism
- Polling databases for new events is inefficient and increases overhead.
- Instead, **event notification mechanisms** (e.g., message brokers) are used to notify consumers when new events arrive.

---

### **Message Brokers & Event Processing**	


 What happens if the producers send messages faster than the consumers can process them? Broadly speaking, there are three options: the system can drop messages, buffer messages in a queue, or apply backpressure (also known as flow control; i.e., blocking the producer from sending more messages). For example, Unix pipes and TCP use backpressure: they have a small fixed-size buffer, and if it fills up, the sender is blocked until the recipient takes data out of the buffer. If messages are buffered in a queue, it is important to understand what happens
as that queue grows. Does the system crash if the queue no longer fits in memory, or does it write messages to disk? If so, how does the disk access affect the performance of the messaging system ?

What happens if nodes crash or temporarily go offline‚Äîare any messages lost? As
with databases, durability may require some combination of writing to disk
and/or replication which has a cost. If you can afford to sometimes lose messages, you can probably
get higher throughput and lower latency on the same hardware.

A number of messaging systems use direct network communication between producers and consumers without going via intermediary nodes:
‚Ä¢ UDP multicast is widely used in the financial industry for streams such as stock
market feeds, where low latency is important. Although UDP itself is unreliable, application-level protocols can recover lost packets (the producer must remember packets it has sent so that it can retransmit them on demand).

Although these direct messaging systems work well in the situations for which they are designed, they generally require the application code to be aware of the possibility of message loss. The faults they can tolerate are quite limited: even if the protocols
detect and retransmit packets that are lost in the network, they generally assume that producers and consumers are constantly online.
If a consumer is offline, it may miss messages that were sent while it is unreachable. Some protocols allow the producer to retry failed message deliveries, but this approach may break down if the producer crashes, losing the buffer of messages that it was supposed to retry.

A widely used alternative is to send messages via a message broker (also known as a message queue), which is essentially a kind of database that is optimized for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker

By centralizing the data in the broker, these systems can more easily tolerate clients that come and go (connect, disconnect, and crash), and the question of durability is moved to the broker instead. Some message brokers only keep messages in memory, while others (depending on configuration) write them to disk so that they are not lost in case of a broker crash. Faced with slow consumers, they generally allow unbounded queueing (as opposed to dropping messages or backpressure), although this choice may also depend on the configuration.

A consequence of queueing is also that consumers are generally asynchronous: when a producer sends a message, it normally only waits for the broker to confirm that it has buffered the message and does not wait for the message to be processed by consumers. The delivery to consumers will happen at some undetermined future point in time‚Äîoften within a fraction of a second, but sometimes significantly later if there is a queue backlog.

Databases usually keep data until it is explicitly deleted, whereas most message brokers automatically delete a message when it has been successfully delivered to its consumers. Such message brokers are not suitable for long-term data storage.
Since they quickly delete messages, most message brokers assume that their working set is fairly small‚Äîi.e., the queues are short. If the broker needs to buffer a lot of messages because the consumers are slow (perhaps spilling messages to disk if they no longer fit in memory), each individual message takes longer to process, and the overall throughput may degrade

This is the traditional view of message brokers, which is encapsulated in standards
like JMS and AMQP and implemented in software like RabbitMQ,ActiveMQ, IBM MQ, Azure Service Bus, and Google Cloud Pub/Sub


#### **Advantages of Message Brokers:**
- Allows producers and consumers to operate independently (handle disconnects, crashes, etc.).
- Supports **asynchronous processing**, meaning producers send messages without waiting for consumers to process them.
- Some brokers store messages in memory, while others persist them to disk for durability.

#### **Message Processing & Delivery Models**
1. **Load Balancing**
   - Messages are distributed among multiple consumers to parallelize processing. 
	- With Each message delivered to only one of the consumer
   - Useful for workloads where each message is expensive to process.

2. **Fan-out**
   - Each message is delivered to **all** consumers, allowing multiple independent systems to process the same data.
   - Equivalent to multiple batch jobs reading the same file.
   
 The two patterns can be combined: for example, two separate groups of consumers
may each subscribe to a topic, such that each group collectively receives all messages,
but within each group only one of the nodes receives each message. Concept of consumer groups  

3. **Acknowledgments & Fault Tolerance**
   - Consumers must **acknowledge** messages to confirm processing.
   - If a consumer crashes before acknowledging, the broker redistributes the message to another consumer.
   - This can lead to **message reordering**, which is problematic for dependent events.

---

### **Log-Based Message Brokers (Hybrid Model)**
- Combines the **durable storage** of databases with the **low-latency notifications** of messaging systems.
- Uses a **log structure** (append-only sequence of records) for storage and retrieval.
- **How it Works:**
  - Producer appends messages to a log.
  - Consumers read messages sequentially from the log.
  - If a consumer reaches the end, it waits for a notification about new messages (similar to `tail -f` in Unix).
- **Scalability:**
  - Uses **partitioning** to distribute messages across multiple machines.
  - Each partition maintains a **monotonically increasing sequence number (offset)** to preserve message order within that partition.
  - **No ordering guarantee across partitions.**
- **Performance:**
  - By partitioning and replicating messages across multiple nodes, modern brokers can handle millions of messages per second while maintaining fault tolerance.

---

### **Key Takeaways**
- **Batch processing** works with bounded data and scheduled jobs, while **stream processing** handles real-time, unbounded data.
- **Message brokers** act as intermediaries to manage event-driven communication.
- **Log-based brokers** provide durable storage with high throughput and low-latency notifications.
- Scalability is achieved through **partitioning** and **asynchronous processing**.

This version makes the notes more structured, concise, and readable while preserving all essential details. Let me know if you'd like further refinements! üöÄ