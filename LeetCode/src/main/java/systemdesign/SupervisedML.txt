Supervised Learning Foundations aims to help learners understand how to build models to 
capture the relationships between variables and a known outcome(historical data) that is continuous in nature,
 how to check the statistical validity of the model, and how to gain business insights from the model.
 
For eg
	the relationship between the sales of a company and the amount spent on advertising?
	Machine Learning
		Learning about the world from the data
		The past is a good representation (information+noise) about the future
		Looking at the past, making an assumption that at least a part of the past(information) will repeat itself
		How can we create a mathematical model out of this that captures only the information leaving out the noise
		The challenge is to identify the information content and distill away the noise
		Underfitting
			Capture much lesser information as given as it should have for prediction
		Overfitting
			Capture all information+ also end up capturing noise
			Does well on training data(because it tries to fit everything including noise) but poor with testing data
		Making fine balance between overusing the past data and overfitting and under using the past data and underfitting it
	
	Data Science
		Learning from data	
		What is data?
			Observations from real world that are very precise
		If we start from precise observations/data and we finish with what we understand of the real world from the data
			A summary of how we believe the world works like starting from the data
			Describe in very very precise terms--mathematical model
		All tools that go to generate the summary: statistics, data mining & ML is called data science
		ML says I am going to try various different models ..try each model on same data is ML and differentiates from standard statistical techniques
		Overlap between ML techniques and statistical techniques
		Split into 2 sets
			Training Set
			Testing Set
	Types of ML tools	
		Supervised Learning	: Building a model using data that contains both inputs and desired outputs(ground truth/labels)
			We have a desired o/p.. everytime we come up a model and compare predicted salary vs desired salary
			Being able to recisely evaluate how good or bad your algorithm is using prediction errors
				Determine if a client will default on a a loan
				Semi Supervised : Some images have labels
			Some supervised learning problems	
				Classification: desired o/p is category	
				Regression: desired o/p is a continuos number
		Unsupervised Learning: We do not have a specific desired o/p-- create segments of customer base to target ads..market segmentation
			We have a set of columns and we group these rows into segments
			No set way to evaluate model performance(no ground truth ..you dont have established customer segmentations)
			Find structure in data like grouping or clustering of points
			Some Unsupervised Learning problems	
				Clustering
				Dimensionality Reduction(reduce size of file without throwing information)
				Association Rule Learning
	Linear Regression				
		Linear relation between 2 variables
		Association is strength of relationship between 2 variables
		Covariance for 2 variables divide by (n-1) for correction
		Var(x)=Cov(X,X);Cov(X,Y)=Cov(Y,X)
				Depends on units so difficult to determine strong or weak covariance
		Correlation
			Measures a linear relation(quantify or measure the defree of association)
			Scale invariant
			Cov(X,Y)/Std(X)*Std(Y)
			Highest positive correlation =+1
			Corr(X,X)=1, Corr(X,-X)=-1.. Therefore correlation is always between +1 and -1
			Makes comparison easy because it is independent of unit(scale invariant)
			If corr(x,y)=0 meaning there is no linear relationship..but there can be other relationships
			Covariance does not imply causation
		Variance and Standard Deviation summarize how close each observed data value is to the mean value. 
		Therefore, if all values of a dataset are the same, the standard deviation and variance are zero.	
	Standard Error
		Standard deviation of residuals
		R2 
			Fraction of variance in y explained by variance
			Between 0 and 1
		Correlation helps us realize the strength of the linear relationship between two variables.
		And, the R-squared value is calculated by squaring the coefficient of correlation.	
		The R-squared value tells us the proportion of variance of the dependent variable explained by the model. 
		For an R-squared value of k, 100 times k percent of the variance is explained. So, for an R-squared value of 
		0.85, 85% of the variance is explained by the model.
	Multiple Regression
		Many independent features
		The decrease in 	 R-squared value signifies that the newly added variable 
			does not add value to the model performance. And hence should not be included.
		The R-squared value increases with the addition of features, but the adjusted R-squared value might increase/decrease 
		with the addition of features to generate the best fit line.	
	One hot encoding
		Categorical variables can't be used directly in a linear regression because
			the best fit lines needs to be fit on numerical values and categorical values represent categories
		Include n-1 coefficients for categorical variables
		# drop_first=True will drop one of the three origin columns
		drop_first=True instructs pd.get_dummies() to drop the first variable after arranging the dummy variables in increasing alphabetical order.
			cData = pd.get_dummies(cData, columns=["origin"], drop_first=True)
			cData.head()
	RMSE and MAE
		Absolute values are very hard to differentiate thats why RMSE is popular for mathematical convenience
	Performance measures should be compared on testing data
	Cross Validation->Automatic way of creating test data
	Cons of Linear Regression	
		Sometimes its too simple to capture real world complexities
		It assumes independence of attributes
		Values like 'america' cannot be read into an equation.
		Using substitutes like 1 for america, 2 for europe and 3 for asia would end up implying that European cars 
		fall exactly half way between American and Asian cars! We don't want to impose such a baseless assumption!
	
	cData["CHAS"] = cData["CHAS"].replace({1: "yes", 0: "no"})
	# For randomized data splitting
		from sklearn.model_selection import train_test_split
	# To build linear regression_model
		import statsmodels.api as sm
	
	# independent variables
		X = cData.drop(["mpg"], axis=1)
	# dependent variable
		y = cData[["mpg"]]

	# let's add the intercept to data
		X = sm.add_constant(X)	
	#Scikit-Learn	
		X_train, X_test, y_train, y_test = train_test_split(
		X, y, test_size=0.30, random_state=1)	

	We will use the OLS function of the statsmodels library to fit the linear model.
		olsmod = sm.OLS(y_train, X_train)
		model = olsmod.fit()	
		print(model.summary())				
	#Coefficient of Determination
		print("The coefficient of determination (R-squared) is ", model.rsquared)
	#Get the predictions on test set
		y_pred = model.predict(X_test)
		y_pred.head()
	#Calculate MSE for training set
		# To check model performance
			from sklearn.metrics import mean_absolute_error, mean_squared_error
		print("The Mean Square Error (MSE) of the model for the training set is ",
		mean_squared_error(y_train, model.predict(X_train)),
		)	
	# we can also get the MSE by the mse_resid parameter of model
	# note that the value will differ slightly due to the consideration of degrees of freedom
		print(
		"The Mean Square Error (MSE) of the model for the training set is ", model.mse_resid
		)
	#Calculate RMSE for training set
		print("The Root Mean Square Error (MSE) of the model for the training set is ",np.sqrt(mean_squared_error(y_test, model.predict(X_test))),
		)
	# Let us write the equation of linear regression: Automate the equation of fit
		Equation = "Price ="
		print(Equation, end=" ")
		for i in range(len(X_train.columns)):
			if i == 0:
				print(model.params[i], "+", end=" ")
			elif i != len(X_train.columns) - 1:
				print(
					"(",
					model.params[i],
					")*(",
					X_train.columns[i],
					")",
					"+",
					end=" ",
				)
			else:
				print("(", model.params[i], ")*(", X_train.columns[i], ")")
			

	

	
	The statsmodels OLS summary includes the R-squared, adjusted R-squared, and model coefficient values. It does not include the RMSE.
	Consider the linear regression equation y = a1x1 + a2x2 + a3x3, where x1, x2, and x3 are three different features. 
	When we use .fit(), the values of the coefficients a1, a2, and a3 are calculated, and when we use .predict(), 
	the feature values for every particular row is used to calculate y for that row. So .predict() gives y value as output for each row.
	
	SST=SSR(sum of squares due to regression,	explained variability)+SSE(sum of squares of error) (unexplained variability)
	
Week 2	
	Statistician vs Machine learning
		y is observed and y hat is what our regression line predicts
		Entire linear regression problem: minimize sum(yi-yhati)^2	yi=>actual , yhati=predicted
		Both Stat'icians are ML'earners are interested in understanding the world from the data
		Stats: A statistician assumes a data-generating model before beginning the analysis.... ML:no data generating model
		Statisticians are mostly interested in determining the performance of a model on the in-sample data, 
		while machine learners have to check the model performance on the out-of-sample data too to compare 
		multiple models and check for overfitting.
		Stats: Can do stats inference ( alpha and beta; y=alpha+beta*x+epsilon) ML:None
		Stats:Field of math ML: Field of CS
		Fit a model->learn, parameter->weight, covariable->feature
		Statisticians are more interested in interpretation*they tend to gravitate towards simpler models),
		Machine learners are more interested in prediction
	Assumptions: Linearity, Independence, Normality, Homoscedasticity
		Linearity: Independent and dependent variables are linearly related
		Independence: Residuals are independent ..epsilon= yi-yhati
		Homoscedasticity: Equal variances of residuals
		Normality: Residuals are normally distributed
	Check for linearity:After finding best linear fit, plot of residuals will provide a good insight
		Lack of pattern of residuals is evidence that the model is linear
	Check for independence: Lack of pattern in residuals meaning one doesnt influence the other
		The assumption of independence is said to be satisfied if the plot of residuals against the predicted values 
		shows no pattern and the points are randomly scattered.
	Check for Normality: Plot histogram of epsilon
		Q Q plot Residuals against perfectly normal curve
			Two distributions are said to be close to each other if their respective percentiles when drawn on a Q-Q plot, lie on a 
			diagonal 45 degree straight line
		Run a hypothesis test like the Shapiro's test
	Check for homoscedasticity
		In case the residuals of linear regression form a funnel-shaped pattern(or others), they are said to be Heteroscadistic
		Identify the cause of Heteroscadisticity
		GQ test( Goldfeld-quandt test)
	No or little multicollinearity-> Two or more independent variables have little to none correlation
		Bad for purpose of interpretation if there is multicollinearity
		Pair correlation matrix for all x values
		Use variance inflation factor for every column.. if value is high then drop, if 0 then independent column
			Use R squared for computing the variance inflation factor of the kth predictor using the remaining k-1 predictors
			Suppose the VIF of a predictor variable is 1.8. Then the variance of the coefficient corresponding to that predictor variable 
			is 80% greater than what it would be if there was no multicollinearity.
			1 then there is no correlation between variables, if exceeds 5 then there is moderate VIF and high if its exceeding 10
			shows signs of multi-collinearity
		Dimensionality reduction(like PCA) help reduce multicollinearity
	Statistical inference from Linear regression
		if B2 was 0 what would be the probability that you see the coefficient is the p value
		null hypothesis is B2 is 0..i.e the independent variable does not affect the outcome or dependent variable
		if p-value<alpha cant be dropped.. (reject the null hypotheses)
		if p-value>alpha we can drop it.. (accept the null hypotheses)
	Hands on
		Calculate VIF for each variable..find ones with high values
		When two or more variables have high VIF, a good approach is to check the effect of dropping each variable individually 
		on model performance and then choose which one to drop. Generally, the one having the least impact on model performance is dropped.
		Drop them one by one and see the effect on R square and adjusted Rsquared
		if Rsquare remains unaffected or increases and adjusred Rsquared increases then drop the column
		After this find p value..drop high p values
		For linearity find residuals and plot them
			Normal distribution->Shapiro Wilk.. observations and qq plot...
			Since p value less than 0.05
			If the assumption of linearity is not satisfied, we can transform one or more variables and 
			check if the transformed variable(s) have a linear relationship with the target or not. 
			We can also transform the target variables if needed.(For eg we made a square of the weight)
			The idea behind keeping both the original and the transformed variables is to let the model decide which variable is significant 
			in making predictions.
			As for correlation, the correlation between the original variable and its transformed version will depend on values and the 
			transformation applied. When a variable is transformed, a multicollinearity check can be performed (if needed) to see 
			whether the original and transformed variables are correlated or not and which one has a larger effect on the target variable. 
			Based on these observations, one can decide on the next steps to take
		Homeschedastic 
			Null hypothesis is they are homeschedastic
			If the p-value of the Godlfeldquandt test is greater than 0.05, we conclude that the residuals are homoscedastic.

