-- Delete and truncate difference
DELETE is a reversible DML (Data Manipulation Language) command used to delete one or more rows
from a table based on the conditions specified in the WHERE clause.
Instead, TRUNCATE is an irreversible DDL (Data Definition Language) command used to delete all
rows from a table
DELETE works slower than TRUNCATE  because it logs each row deletion.
 Also, we can't use the TRUNCATE statement for a table containing a foreign key.

 -- Create the 'orders' table (parent table)
 CREATE TABLE orders (
     order_id INT PRIMARY KEY,
     order_date DATE,
     customer_name VARCHAR(100)
 );

 -- Create the 'order_items' table (child table)
 CREATE TABLE order_items (
     order_item_id INT PRIMARY KEY,
     order_id INT,
     product_name VARCHAR(100),
     quantity INT,
     price DECIMAL(10, 2),
     FOREIGN KEY (order_id) REFERENCES orders(order_id)
 );

 orders table: parent table

 order_id	order_date	customer_name
 1	2025-01-01	John Doe
 2	2025-01-02	Jane Smith
 3	2025-01-03	Alice Johnson

 order_items table: child table

 order_item_id	order_id	product_name	quantity	price
 101	1	Laptop	1	1200.00
 102	1	Mouse	2	25.00
 103	2	Keyboard	1	75.00
 104	3	Monitor	1	300.00
 105	3	HDMI Cable	3	15.00

Before truncating the parent table (orders), you can delete the rows in the child table
(order_items) that reference it:

DELETE FROM order_items WHERE order_id IN (SELECT order_id FROM orders);
Now you can truncate
TRUNCATE TABLE orders;

Option 2: Temporarily disable the foreign key constraint
Option 3: Use DELETE instead of TRUNCATE
If you can't or don't want to disable the foreign key check or delete the rows manually,
you can use DELETE to remove all rows from the table, which does check foreign key constraints:
DELETE checks for foreign key constraints before removing a row ensuring data integrity is maintained
 If there are dependent child rows in the referenced table,
the database will either throw an error or delete the child rows based on the foreign key's
 ON DELETE action (such as CASCADE, SET NULL, or RESTRICT).

 TRUNCATE is not designed to respect foreign key constraints at all.
 It quickly drops all rows in the table without any checks,
 which could lead to orphaned records in the child table (i.e., child records that no longer have a valid parent).
  Because of this, if there is a foreign key relationship,
  TRUNCATE may breaking data integrity.

-- -- Drop and truncate difference
DROP deletes a table from the database completely, including the table structure and all the
associated constraints, relationships with other tables, and access privileges.
TRUNCATE deletes all rows from a table without affecting the table structure and constraints.
DROP works slower than TRUNCATE. Both are irreversible DDL (Data Definition Language) commands.

- Having works on aggregated data after they are grouped, while 'WHERE' checks each row individually.
 If both statements are present in a query, they appear in the following order:
 WHERE – GROUP BY – HAVING. The SQL engine interprets them also in the same order.

 20. What is normalization in SQL, and why use it?
Normalization is a systematic process of organizing data in a relational database to minimize redundancy
(duplicate data), eliminate undesirable characteristics like insertion, update, and deletion anomalies,
 and ensure data integrity. It involves decomposing a table into smaller (and less redundant) tables
 and defining relationships between them.
 The process typically follows a series of normal forms (1NF, 2NF, 3NF, BCNF, etc.),
 each addressing specific types of redundancy and dependency issues
-- By organizing data into multiple related tables, normalization eliminates duplicate information,
 ensuring that each piece of data is stored only once.
 With reduced redundancy, the risk of data anomalies (inconsistencies arising from update,
 deletion, or insertion operations) decreases.
 Data integrity rules (like primary keys, foreign keys, and constraints)
 help maintain accuracy and consistency across the database

OrderID	CustomerName	CustomerAddress	OrderDate
1	    John Doe	    123 Main St	2020-01-01
2	    John Doe	    123 Main St	2020-01-02
3	    Jane Smith	    456 Elm St	2020-01-01

After Normalization (Split into Two Tables):

Customers Table:
CustomerID	  CustomerName	CustomerAddress
1	           John Doe	    123 Main St
2	           Jane Smith	456 Elm St

OrderID	CustomerID	OrderDate
1	1	2020-01-01
2	1	2020-01-02
3	2	2020-01-01
Improved Data Integrity: Updates to customer information need to be made in only one place.
Easier Maintenance: Fewer chances of data inconsistencies.

While normalization is aimed at reducing redundancy and ensuring integrity, denormalization involves
intentionally reintroducing redundancy by combining data from multiple tables.
Denormalization is often employed when read performance is prioritized over write performance
because:
Faster Query Performance:
It can reduce the complexity and number of joins needed to retrieve data,
thereby speeding up query execution times.

37. How to find the nth highest value in a column of a table?
SELECT * FROM EMPLOYEE ORDER BY SALARY DESC LIMIT 1 OFFSET 5;

DDL: CREATE, ALTER TABLE, DROP, TRUNCATE, and ADD COLUMN
DML: UPDATE, DELETE, and INSERT
DCL: GRANT and REVOKE
TCL: COMMIT, SET TRANSACTION, ROLLBACK, and SAVEPOINT
DQL: – SELECT

(INNER) JOIN – returns only those records that satisfy a defined join condition in both (or all)
 tables. It's a default SQL join.
LEFT (OUTER) JOIN – returns all records from the left table and those records from the
right table that satisfy a defined join condition.
RIGHT (OUTER) JOIN – returns all records from the right table and
those records from the left table that satisfy a defined join condition.
FULL (OUTER) JOIN – returns all records from both (or all) tables.
It can be considered as a combination of left and right joins.

Type of Index
A Unique Index ensures that no duplicate values exist in a specific column of a table.
Helps maintain data integrity by preventing duplicate entries.
--If we try to insert another row with an existing Email, the database will throw an error:

2. Clustered Index
A Clustered Index defines the physical order of rows in a table.
A table can have only one clustered index because it determines how data is physically stored.
By default, the Primary Key creates a Clustered Index.
CREATE CLUSTERED INDEX idx_order_id
ON Orders (OrderID);
The database physically arranges records in the order of OrderID.
When we query data, searching by OrderID is faster because the records are already sorted.
When you create a table with a Primary Key, a Clustered Index is automatically created.
If your table doesn’t have a Primary Key, you might need to manually create a Clustered Index on a column
that is often used for searching or sorting


3. Non-Clustered Index
The data remains stored in its original order.
Non clustered stores pointers to the actual row locations.
A table can have multiple non-clustered indexes
Stores pointers for fast lookups without changing data order
EmployeeID	Name	Department	Salary
1	Alice	HR	60000
2	Bob	IT	75000
3	Charlie	HR	62000

CREATE NONCLUSTERED INDEX idx_department
When we search for employees in HR, the database looks up the index instead of scanning the full table.
✅ Non-clustered indexes improve search efficiency and allow multiple indexes per table.


Inserting 100 million records into a database is a huge data operation and
requires careful planning to ensure that the process is efficient,
does not negatively impact the performance of the database, and handles potential errors or
timeouts effectively. Here are some best practices to follow when performing such a
large-scale insert:

1. Bulk insert API/Data import tools
    Use data import tools when dealing with a one-time migration of structured data from a file.
    Optimized for high speed data ingestion
    LOAD DATA INFILE (MySQL) or COPY (PostgreSQL) to import the first batch.

2)  Batch Insert:
    Batch Insertion: Rather than inserting each record individually,
    you should insert data in batches (e.g., 10,000 or 100,000 records at a time).
    This reduces the number of transactions and is much more efficient.
    Allows transactional control, meaning inserts can be rolled back if needed.

    INSERT INTO students (id, name, email)
    VALUES
        (1, 'Alice', 'alice@email.com'),
        (2, 'Bob', 'bob@email.com'),
        (3, 'Charlie', 'charlie@email.com');

        Using prepared statements with batch execution.

PreparedStatement stmt = connection.prepareStatement(
    "INSERT INTO students (id, name, email) VALUES (?, ?, ?)");
connection.setAutoCommit(false);

for (int i = 1; i <= 1000000; i++) {
    stmt.setInt(1, i); // Use 'i' as ID if no array
    stmt.setString(2, "Student" + i);
    stmt.setString(3, "student" + i + "@example.com");
    if (i % BATCH_SIZE == 0) { // Execute batch when size reaches BATCH_SIZE
           stmt.executeBatch();
           conn.commit(); // Commit after batch execution
     }
}

✅ Set BATCH_SIZE manually based on your database performance (e.g., 1000, 5000).
✅ Use addBatch() and executeBatch() to process records in chunks.
✅ Commit after each batch to reduce memory usage and improve speed.
Use only bulk inserts when inserting records programmatically and frequently,
such as in an ETL process or Kafka consumer.

A good rule of thumb is to commit after every 10,000 to 100,000 records,
depending on the system and the transaction log overhead.

Hybrid Approach Example
If your system frequently receives new records via an API but also needs an initial large dataset:
Step 1: Use COPY (PostgreSQL) to load existing data from a CSV.
Step 2: Use batch inserts in Java to handle incoming data.

2. Indexes: Indexes slow down inserts because the database must update them every time a record
is inserted. If possible, disable indexes before performing the insertion and rebuild them afterward.

3. Disable Foreign Key Constraints Temporarily (If Safe):

4. Use Parallel Inserts:
Multi-threading or Parallel Processing: Depending on your database,
you might be able to use multiple threads or processes to insert data in parallel.
Be cautious, as this can lead to locking or contention problems if not done correctly.
If you're using an application to perform inserts, you can split the records into smaller chunks
and insert them in parallel from different threads or processes.

5. Data Validation and Cleansing:
  Before performing the insert, validate the data to ensure there are no inconsistencies
  or errors (e.g., duplicate entries, invalid formats).
  Pre-validating data reduces the likelihood of failed inserts or database errors.

6.Optimize Database Configuration:
  Increase buffer pool size: Make sure the database is using an optimal amount of memory
  for data caching. You might want to increase the buffer pool size so that more data can be stored
   in memory during inserts.
  Increase transaction log settings: If the transaction logs are set to a low size,
  increasing the log buffer can reduce the I/O overhead of committing transactions.




Specifically, when dealing with database transactions, transaction log overhead involves the following:

1. Transaction Logs in Databases
Every time you execute an insert, update, or delete operation, the database records the changes
in a transaction log.The transaction log helps ensure ACID properties (Atomicity, Consistency, Isolation,
Durability) in case of crashes or failures, so it records every transaction to guarantee that
changes can be rolled back or replayed.
2. Overhead in Context
When you commit a batch of records (for example, every 10,000 or 100,000 records),
the database writes these changes to the transaction log.
Writing to the log is I/O (Input/Output) intensive and adds extra processing time.
The overhead here refers to the extra time and system resources the database spends
writing to the transaction log after each commit.
The more often you commit (e.g., after every 1 record or small batch),
the more frequent these logs are written, causing additional load on the database system.
3. Effect of Overhead
High Overhead: Committing very frequently (after every 1-10 records)
means the database has to constantly write to the log, slowing down performance,
especially if there are millions of records.
Low Overhead: By committing after larger batches (e.g., 10,000–100,000 records),
you reduce the number of times the database writes to the log, making it more efficient,
as fewer commits need to be processed.
4. Trade-Off
Smaller Batches (Frequent Commits): This can give you higher data integrity in case of failures,
but at the cost of higher overhead.
Larger Batches (Less Frequent Commits): This reduces the overhead of writing to the transaction log,
but increases the risk of larger rollbacks if something goes wrong.
It also puts more load on the database during the commit.
Conclusion:
Committing in smaller batches reduces transaction log overhead but increases the frequency of writes,
while committing in larger batches minimizes transaction log writes but increases
the risk of heavier operations in case of errors. Balancing batch size based on the
system and workload is important for optimal performance.








You said:
does the content of transaction log change depending on smaller or larger batch
ChatGPT said:
Yes, the content of the transaction log can change depending on whether you use smaller or larger batches when committing data to the database. Here’s how it works:

1. Smaller Batches (Frequent Commits)
More frequent entries in the transaction log: When committing smaller batches,
each batch will result in a transaction log entry. Each transaction log entry will record the changes made by that particular commit.
For example, if you're committing 1,000 records in a batch, each batch commit will be logged separately, and the transaction log will contain more entries.
More detailed logging for each commit: Smaller commits may involve more granular details in the transaction log. Each commit will contain the changes made for that batch, along with the associated metadata (like transaction ID, timestamps, and the operations performed).
2. Larger Batches (Less Frequent Commits)
Fewer entries in the transaction log: With larger batches, there will be fewer commits, so fewer entries will be added to the transaction log. Each commit will record a larger set of changes in one log entry.
For instance, if you're committing 100,000 records in a single batch, there will be just one log entry for that entire batch.
Less granular logging: Since the commit is larger, the log will record the changes made for a larger set of records in a single entry, which may not be as detailed as smaller commits. It will record the set of operations but won't have separate entries for each individual record change.
3. Transaction Log Content
Regardless of whether the batch size is small or large, the transaction log will typically contain:

Transaction start and end: Each commit or batch will have a start and end in the log.
The changes (inserts, updates, deletes): The actual data modifications are written to the log.
Undo/Redo information: In case of rollback or recovery, the log stores the necessary information to reverse the changes or apply them again.
Metadata: Information like timestamps, transaction ID, and status of the transaction will be logged.
Impact of Smaller vs. Larger Batches on the Log
Smaller Batches: More frequent commits mean more entries in the log and potentially higher overhead. Each log entry will be smaller, as it only contains the changes for that specific batch.
Larger Batches: Fewer, larger log entries mean reduced overhead in terms of the number of entries, but each log entry will be larger since it contains information about a bigger batch of data.
Key Points
Transaction log content: The content (changes, undo/redo info) remains similar, but the frequency and size of log entries differ.
Smaller batches → More frequent, smaller log entries.
Larger batches → Fewer, larger log entries.
Thus, the content of the transaction log doesn’t fundamentally change (i.e., it still records the changes), but the volume and size of the log entries are affected by the batch size.


Transaction logs play a critical role in ensuring atomicity, consistency,
and durability—the ACID properties of database transactions.

Here’s how the flow works:

1. Start of Transaction
When you initiate a DML operation (e.g., INSERT INTO, UPDATE, DELETE), the transaction begins.
If you're running a single query, the transaction might start implicitly,
but in some cases, you may explicitly begin the transaction (e.g., using BEGIN TRANSACTION).
The database creates an internal transaction context to track all changes made by this operation.
2. Transaction Log Records the Operation
Before the changes are applied to the database, the database system first writes the
DML operation into the transaction log (also called redo log in some systems).
The transaction log is a sequential record of all changes to the data,
ensuring that if the transaction fails or the system crashes,
the database can recover to a consistent state.
Write-Ahead Logging (WAL) ensures that the log entry is written before the actual change is
 applied to the data files. This is key for durability and crash recovery.

Example of Log Entry for INSERT:
Transaction ID: 12345
Operation: INSERT
Table: students
Values: (1, 'John Doe', 'john@example.com')
This log entry records the action being taken, including the specific table and data,
so that it can be undone or redone later during recovery if necessary.

3. Change is Applied to the Database (Buffer Cache)
After the transaction log is updated, the actual DML operation is applied to the buffer cache
or memory cache (not directly to disk yet). This is a temporary storage area in memory where
data changes are staged.
The database updates the data in memory, but the changes are not yet persistent on disk.
They exist in the buffer cache until the database is ready to commit them to disk.
At this point, the changes have not yet been written to the actual database file (disk storage).
4. Transaction Commit or Rollback
Commit: If the transaction is successful,
the changes in the buffer cache are written to the actual database files (on disk) in a process
known as checkpointing.
After this, the transaction is marked as complete, and the database commits the changes to disk.
The transaction log is also marked with a commit record, indicating that the transaction was
successfully completed.
Once committed, the changes are durable and cannot be undone, even if the system crashes later.
The database detects an error (constraint violation, invalid ID, etc.).
Since the transaction is still active and hasn't been committed, all changes exist only in memory.
The database does not write changes to disk yet.

If an error occurs, the transaction can be rolled back.
The database uses the transaction log to undo any changes made during the transaction,
ensuring that the system remains in a consistent state.
5. Write-Ahead Logging (WAL) & Durability
WAL Protocol: The Write-Ahead Logging (WAL) protocol ensures that no changes are applied
to the data files unless the corresponding log entry has been recorded first.
This means that even if the system crashes after the transaction log entry
but before the data is written to disk, the database can use the log to redo the operation
when the system restarts, ensuring the database's durability.

Example Flow:
You run a DML query like:
INSERT INTO students (id, name, email) VALUES (1, 'John Doe', 'john@example.com');

Step 1: The database writes the following log entry to the transaction log (WAL):
[Log] INSERT INTO students (id, name, email) VALUES (1, 'John Doe', 'john@example.com');

Step 2: The database applies the changes to the buffer cache (in-memory).

Step 3: The database performs a commit:

The log is updated with a commit record:
[Log] COMMIT (Transaction ID: 12345)
The changes in the buffer cache are written to the actual data files (disk).

Step 4: The operation is complete, and the changes are now permanent.

Why Transaction Logs are Critical:
Recovery: The transaction log ensures that, in case of a crash,
the database can recover all committed changes and undo incomplete transactions.
This ensures atomicity and consistency of the database.
Durability: Even if a power failure happens after a commit but before the data is written to disk,
the database can still restore the changes from the transaction log.
Performance: Writing to the transaction log is fast, which allows the system to commit changes
 quickly without waiting for the slow disk writes.
Conclusion:
In summary, the transaction log is written first, followed by applying the changes to the
in-memory data (buffer cache). If everything goes as planned, the transaction is committed, and the changes are then written to the disk. This ensures consistency, durability, and crash recovery, while maintaining efficient system performance.







Types of Commit in a Database
 --Explicit Commit (Manual Commit)
    This happens when the user or application explicitly runs a COMMIT statement to finalize
    the transaction.
    Example:
    BEGIN TRANSACTION;
    INSERT INTO students (id, name, email) VALUES (1, 'John Doe', 'john@example.com');
    COMMIT;
    Here, the transaction remains uncommitted (in the transaction log and buffer)
    until the COMMIT statement is executed.
    If something goes wrong before the commit (e.g., an error occurs or the user issues ROLLBACK),
    the changes will not be applied to the database.

--Implicit Commit (Auto-Commit Mode)
    Some databases (like MySQL with default settings) automatically commit every statement
    unless a transaction is explicitly started.
Example:
    INSERT INTO students (id, name, email) VALUES (1, 'John Doe', 'john@example.com');
    In auto-commit mode, this statement is immediately committed after execution without
    requiring a separate COMMIT command.
    However, if auto-commit is disabled, the user needs to explicitly commit the transaction.
    System-Level Commit (Internal Commit by Database Engine)

--Some databases use internal commits for certain operations, like:
    DDL (Data Definition Language) Statements:
    CREATE TABLE, ALTER TABLE, DROP TABLE, etc., usually auto-commit regardless of transaction settings.
    Bulk Inserts and Batch Processing:
    Some databases automatically commit after processing large chunks of data to optimize performance.
    Checkpoints in Database:
    Databases periodically write all committed changes from memory (buffer cache) to disk in checkpoints.

Conclusion
    So, when does the commit actually happen?
    If the user runs COMMIT, then that is when the commit happens.
    If auto-commit mode is enabled, the database engine commits automatically after each statement.
    Some database operations automatically commit due to system behavior (like DDL changes).
    If a transaction is not committed (explicitly or implicitly),
    the changes stay in memory (buffer) and do not persist in the actual database files.




