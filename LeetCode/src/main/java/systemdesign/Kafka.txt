------------Kafka--Section 1 Stephen Maarek

1) Kafka Introduction

If you have 4 source systems and 6 target systems, you have to write 24 integrations
Each integration comes with different complexities
	Protocol--> how data transported TCP,HTTP, REST, FTP,JDBC
	Data format--> Binary, CSV, Python
	Data schema and evolution--> how data is shaped may change ---Data schema and evolution happens over time(data changes in shape both ss and target system?)
Each ss will also have an increased load from all connects and request to extract the data
We bring some decoupling using Apache kafka-->decoupling of data streams and systems
Source data could be 
	--> website events, user interaction, financial transactions, pricing data
Target could be 
	--> databases, analytics,email system and audit, notification service


Kafka created by linkedin now open source project managed by Confluent, IBM, Cloudera
	Distributed, Resilient architecture, Fault Tolerant
	Horizontal Scalability--> Can scale to 100's of brokers, can scale to millions of messages per second
	High performance-->real time system-->low latency less than 10 ms
	Widespread adoption
Graphical UI for Apache Kafka is Conduktor

Apache Kafka Use cases
	Gather metrics from different locations
	Application logs gathering
	Activity tracking
	Messaging system
	Microservices Pub/Sub (Decoupling of system dependencies)
	Stream processing
	Integration with Big Data technologies
Usage in Industry	
	Uber using Kafka to gather trip,user ,taxi data in real time to compute and forecast demand and compute surge pricing in real time
	Netflix for providing real time recommendations while you are watching TV shows
	LinkediN to collect spam, make better connection recommendations in real time
Kafka is used as a transportation mechanism
Architect-->understand the role of Kafka in enterprise pipelines


For developers
	Kafka connect API
	Ksql DB
	kafka Streams API
	Confluent Components

Section 4 Kafka Theory
Topic
	Particular stream of data within a Kafka cluster(logs, purchases, tweets, trucks gps    )... (similar to table in a dabatabase without the constraints)
	Can have many topics.. they are identified by name
	Support any kind of messon format (json,xml,binary)
	Sequence of messages in topic is called data stream
	Cannot query topics...use kafka producer to send data and kafka consumers to read data

Partition: 
	Topic can be divided into partitions (3)
	You can have as many partitions as you want
	Messages sent to topic are going to one of the partition
	In each partion each message is ordered (with the help of id)..
	every message in a partition gets an incremental id called Kafka partition offset
	Kafka topics are immutable..once the data is written to the partition..it cannot be changed
	Once sent to a topic, a message can be modified?
	
Offset
	Multiple services are reading from the same stream of data thats an advantage for eg truck GPS goes to location dashboard and notification service
	Data once written to Kafka topic cannot be changed (immutability)
	Data in Kafka is only kept for limited time (1 week but thats configurable).. 
	offset 3 in partition 0 is different of offset 3 in partition 1
	Offset will not be reused even if previous messages are deleted
	Order is guaranteed within partition but across partition if we need ordering difficult to achieve this ?
	Data is randomly assigned to a partition ***unless the key is provided

Producers 
	They write data to topics(which made to partitions)
	Producers know to which partition to write to and which Kafka broker has it (producer knows/decides in advance)
	In case of Kafka broker failures producers will automatically recover

Key: Producer can add keys to messages.. if key is null, then data is sent round robin
if key!= null then messages for same key will go to same partition for eg truck id..where we want to get continuos set of data
jkey is sent for message ordering
Key binary+Value-Binary+Compression+Headers(optional ) key value pair+partition+offset+timestamp(set by system or user)
This kafka message gets sent to kafka for storage

Kafka Message Serializer
Accept bytes as input from producers and sends bytes as outputs to consumers
We perform serialization..transform data into bytes..used into value and key.. IntegerSerializer and Value serializer

Kafka partioner (code logic that takes a record and determines to which partition to send it to) ..More on this?

Consumers
They read data from a topic (pull model)
A consumer may read data from one or more partition.. consumers know which broker to reader from
They know how to recover
Data is read in order from low to high offsets
Consumer deserialize--transform bytes into object used on both key and value of the message(can be for Integer string Avro protobuf)
Consumer needs to know in advance what is the expected format for your key and value
During the topic lifecycle, once the topic ic created, You must not change the type of data which is sent by the producers
otherwise you are going to change the consumers... You can create a new topic instead.. and reprogram consumers
Consumer group-> A group of consumers who read from the partitions covering a topic
Multiple consumer groups can read from one topic
However within a consumer group, one partition can be assigned to only one consumer
To create distinct consumer groups we will use consumer property group.id

kafka stores offset at which consumer group has been reading..Why consumer group here.. More on this?
The offsets are in the Kafka topic name of __consumer_offsets and are periodically committed
If a consumer dies, it will be able to read back from where it left off thanks to the committed consumer offsets

Delivery strategy-semantics
Java consumer by default will commit at least once (right after the message is processed)
Our processing should be idempotent so reprocessing the message has no impact

At most once--> as soon as me/ssage is received..if processing goes wrong some messages will be lost

Exactly once--->Use the transactional API

Kafka brokers: 
	A Kafka cluster is composed of multiple brokers(server)
	Each broker identified by an id (integer)
	They receive and send data
	Each broker contains certain topic partitions which means all topics are distributed across all brokers-- This is what makes Kafka scale (horizontal scaling)
	
	Broker 101			Broker 102 			Broker 103
	Topic-A Part 0		Topic A Part 2 		Topic-A Part 3
	Topic-B Part 1		Topic-B Part 0

After connecting to one broker..you can connect to any broker (entire cluster) bootstrap broker-- kafka clients have smart mechanics
	Every kafka broker is called a bootstrap server--has knowledge of all the brokers in the cluster
	kafka client will initiate a connection and send metadata request to broker 101 and broker 101 will return list of all brokers
	Kafka client will then connect to the required broker to produce or consume data
	This is how clients connect to a Kafka cluster
	
Topic replication factor
	Every topic has a replication factor
	In prod, replication factor >1 to provide fault tolerance(usually 2-3 but more often 3)
	That means partition are replicated across different brokers depending on replication factor
	This way if a broker is down another partition can serve data for fault tolerance
	
	Broker 101			Broker 102 			 Broker 103
	Topic-A Part 0		Topic A Part 1 		 Topic-A Part 1(repl)
						Topic-A Part 0(repl)
	
	There can be however only 1 leader for a partition at a given time
	Producers will send data to leader to the leader of the partition, the other brokers will replicate the data ..if its fast enough we call it ISR
	Each partition has one leader and multiple ISR

Kafka v2.4( possible to read the data from the closest replica) to improve latency, performance
Producer acknowledgement--> acks=0 producer will not wait for acknowledgement from broker that write happened(possibl data loss)
acks=1 producer will wait for leader to acknowledge(limited data loss)
acks=all (all in sync replicas to provide confirmation) -> no data loss

Kafka topic durability
if you have replication factor of N, you can still loose N-1 brokers and recover your data

Zookeeper
manages brokers(keeps a list of them)
helps in performer leader elections
sends notification to kafka in case of changes( new broker, broker dies, broker comes up, delete topics)

Apache zookeeper used by kafka for storing metadata for the users
Used for leader election
Keeps list of brokers
Sends notification to brokers.. kafka until 2.x cannot work without zookeper
starting with 3.x Kafka raft mechanism , it can work without zookeeper
kafka 4.0 can work without zookeeper
Zookeeper by design operates with an odd number of servers(1,3,5,7)
Zookeeper cluster (1 leader for writes and rest are followers (reads))
Kafka clients were configured to be connected to zookeeper,,before
But now never use zookeeper as a configuration in your kafka clients..over time the kafka clients  and CLI have been migrated to leverage the brokers as a connection
instead of zookeeper
Since Kfka 0.10 offsets are stored in kafka topics and consumer must not connect to zookeeper
Since Kafka 2.2 kafka-topics.sh CLI commands Kafka brokers and not zookeeper for topic management
With more than 100,000 partitions in cluster, zookeeper was having scaling issues
Kafka 3.x implements kraft to replace zookeeper
Zookeeper is less secure and care should be taken to ensure ports are open to allow traffic only from Kafka brokers and not Kafka clients
Basically modern day developer will never use zookeeper as configuration in your kafka clients and other programs that connect to Kafka
 3 zookeepers managing 3 brokers// in new system only 3 brokers, one is designated as leader to replace zookeeper function








Data is needed to be moved between applications and data stores
Apache Kafka--provides messaging infrastructure.. 1.4 trillion messages every day
Data movement/logistics:Get lots of messages from one system to another, rapidly, scalably and reliably
In computing, transfer of data is called messaging but unlike other systems..Kafka is used for
high throughput use cases,,vast amount of data is needed to be moved
**other applications***traditional message systems** struggle with scalability***
Database replication/log shipping (that support replication)
ETL is option..proprietary and costly.Lots of custom development
Apps are publishers... brokers is like mailbox and consumers are like apps who consume
Under higher volumes, blast the broker with messages.. if there is no throttling
message is dropped.. feed it to consumers..lazy slow or unresponsive application consumers
Messaging systems are more of a middleware
Kafka is a distributed messaging system,designed to move data at high volumes
Created to address shortcomings of traditional data movement tools and approaches particulary when the data is growing
and it needs to move faster across more and more diverse systems

***Kafka architecture*******
Publishers, consumers, brokers, topic
Publisher sends messages to a location called topic
Topics have a specific name that can be decided upfront or on demand
As long as producers know the topic name and have perm to send on the particular topic, they should be able to send the messages
Consumer receive messages based on topic they are interested in
The place where kafka keeps and maintains topics is called broker(physical containers of data)
Broker is a executable daemon process that runs on a machine.. physical machine or a virtual machine
(Multiple Throughput)
Broker uses file system of underlying machine
With Kafka, you can scale out as much as brokers to achieve level of throughput
Kafka cluster is a grouping of multiple Kafka brokers(Grouping mechanism that determines a cluster's membership
of brokers is important part of Kafka's architecture and enables scaling)-->Apache Zookeeper?


Kafka..real time decision
make your data move fast at scale

Producer vs consumer (Source vs Sink)
Data driven vs event driven approach
Put events in the queue
Start putting data in queue
Events will be unique in nature..
Message goes under a topic in queue
Topic is a category--- eg customer....
Topic can have multiple partitions? For fault tolerance
Sequence correct within one partition

Kafka cluster contain many brokers
Take message from producer, assigns it to offset and store on local disk..Catch to fetch request from consumer
one partition assigned to multiple broker with owner being leader for one partition
Messaging, Storing and Caching as a part of the core Kafka product

Kafka connect--- Mainframe.. db2
KSQL to run queries
kclient to connect through
streaming data

Loose Decoupling(Small services to be designed and stored in container)
Fully distributed
Easy to Scale




New notes----------******************************************************************************************************

Producers Take data from source systems and send data into apache kafka.. round robin concept key based strategy, acks strategy
data is distributed to different partitions
Consumers operate in consumer group//store offsets in an offset topic (delivery strategy semantics)
Kafka cluster managed by zookeeper..leader followe broker management
To produce data to a topic, a producer must provide the Kafka client with any broker from the cluster and the topic name
Very important: you only need to connect to one broker (any broker) and just provide the topic name you want to read from.
Kafka will route your calls to the appropriate brokers and partitions for you!

How we move data becomes as important as the data itself
data is at the core of making decisions, the faster, easily we move data, the more agile
our organizations can be and the more we can focus on customer needs
our organizations can be and the more we can focus on customer needs
Kafka is an example of publish Subscribe messaging system
Enter Kafka: Unit of data is called a message. A message is an array of bytes without structure
A Message can have optional metadata called as Key..The key also is byte array and as with the message
has no specific meaning to kafka
Key is used when data needs to be written in more controlled manner?(Multiple partition)
For efficiency messages are written into Kafka in batches
A Batch is a collection of messages all of which are being produced to the same topic and partition
Trade off between latency and throughput

Schema: Schema additional structure or schema imposed on messaged content so that it is easily understood
(json) XML (extensible markup language)
Use well defined schema and Put data format in a common repository, messages can be understood
without coordination( Decoupling)

Topics: Messages in Kafka are subscribed into topics. Topics are additionally broken down into partitions
A partition is a single log. Writes are appended towards the end
Partition is how Kafka provides redundancy and scalability. Partitions can be hosted on different server
So single topic can be scaled horizontally across multiple servers

Stream: is considered to be a single topic of data regardless of the number of partitions
Single stream of data moving from producers to the consumers

Kafka Clients: Users of the system and there are 2  basic types producers and consumers
A message will be produced to a specific topic..by default the producer will balance messages to all partitions
or to a specific partition using message key (More on this)

Consumers read messages.. In other publish subscribe systems they are called subscribers or readers
Consumer subscribes to one or more topics and reads the messages in the order in which they were
produced to each partition...Consumers keeps track of the messages by using offset
Each message has a unique offset in a given partition.
Kafka creates a monotonically increasing value(meta data)
called offset to each message as it is produced..consumer use this offset and they can stop and restart
without loosing its place

Consumer group : One or more consumers work together to consume a topic
Each partition is consumed by only one member(more on this)


Broker: A single Kafka server is called a broker
Broker receives messages from producers, assigns offset to them and writes the message to storage on disk
It also services consumers responding to fetch requests from partition and responding with the messages
that have been published
Single broker can handle thousands of partitions and millions of messages per second

Kafka cluster: Kafka Designed to operate as a part of cluster
One broker will function as the cluster controller and responsible for administrative operations
including dealing with failures and assigning partitions to brokers
A partition is owned by one broker and that broker is called leader of the partition
A replicated partition is assigned to additional brokers called followers of the partition
For eg in figureL Broker 1 leader of partition 0, Broker 2 is follower of partition 0
All producers must connect to the leader in order to publish messages but consumers may fetch message
from leader or followers
Retention: Based on strategy (either time or partition reaches a certain size)

Multiple kafka clusters? Mirror maker

Why Kafka?Among multiple publish/subscribe systems
Kafka can handle: Multiple producers whether clients are using many topics or same topic
kafka can handle multiple consumers to read single stream of message without interfering with each other
Disk based retention: Messages are written to disk
Scalable: Scalability makes it easy to handle any amounts of data
Can start with a single broker, move to production with large clusters of tens of hundreds of brokers
Expansion can be done while cluster is online with no impacts to availability
High performance:
Streaming is easy
The data eco system :

Usecases
(1) Activity tracking; User clicks on different frontends generates messages related to various topics
And the consumers listen to these topics for generating reports, feeding ML system ,updating search results etc
(2)Messaging: Applications need to send notifications
(3) Metrics and logging

kafka is based on the concept of a commit log?
Name based on Franz Kafka founded in 2010


Kafka CLI comes bundled with the kafka binaries
kafka-topics.sh
replication factor can be 1 for 1 broker
kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --create --partitions 1 --replication-factor 1
 kafka-topics.sh --bootstrap-server localhost:9092 --list
 kafka-topics.sh --bootstrap-server localhost:9092 --describe
 kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --delete
 kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first_topic

 null keys// only values
 but you can produce keys
  kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first_topic --property parse.key=true --property key.separator=:

